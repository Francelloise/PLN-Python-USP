# -*- coding: utf-8 -*-
"""Aula python e PLN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iRRjyr9WAlnGi0MCqdHddaSYuzmx3kCa
"""

print("Hello World!")

"""#Aula 3 - Introdução ao Python"""

a = "Francelloise aluna de Python"
print(a)

# Quando eu tenho uma dúvida sobre qual o tipo da varável, caso eu tenha dúdiva em que tipo ele seja usamos type
type(a)

for i in range(5):
  print(i)

lista = []
lista = [0, 1, 2, 3, 4]
lista = list(range(10))
print(lista)

lista.append(120) # Adiciona um objeto ao final da lista, nesse caso adicionei o 120 e a string fim
print(lista)

# Trocando o segundo ou outro elemento da lista determino qual a posição quero mudar e o valor dela
lista[2] = 4
lista[6] = 16
lista[11] = 11
lista[12] = 12
print(lista)

# Inserindo um objeto/elemtento em determinada posição da lista
lista.insert(6,8)
print(lista)

# Revertendo a minha lista
lista.reverse()
print(lista)

# Ordenando a lista, colocando os elementos em ordem crescente
lista.sort()
print(lista)

# Contando os elementos da minha lista
lista.count(12)

# Mostrando em que posição da lista está determinado elemento, se houver elementos repetidos, mostrará somento o primeiro.
lista.index(8)

# Fatiando minha lista de um ponto ao outro
lista2 = list(range(8))
print(lista2)
print(lista2[2:7]) #fatiando da posição 3 ao 7 [2:7]
print(lista2[2:7:2]) #fatiando da posição 3 ao 7 mas pulando os elementos de 2 em 2

# Ainda usando o sort(), colocando a lista em ordem alfabética
lista3 = ["bolo", "maizena", "água", "ovos", "trigo", "leite"]
lista3.sort()
print(lista3)

# Criando tuplas, diferente das lisas elas ficam entre parenteses () e os lementos não podem ser modificados
tupla = (1, "bolo", 2, "torta", 1.2, "sobremesa", 3)
print(tupla)
print(type(tupla))

l2 = [["Ana", 10, 1,30], [2, "bolo", "ovos"], ["Fran", 3, "Aula"]] # Dicionários é como se fossem uma lista dentro de outra lista

# Acessnado o elemento da lista anterior, diferente do dicionário que é mais fácil
l2[1][2] # Aqui eu selecionei a segunda lista da lista, e o 3º elemento dessa segunda lista

# Dicionário, definindo o dicionário e acessando seus elementos, é muito mais fácil do que no exemplo anterior
dic = {}
dic["Ana"] = 10
dic["Bolo"] = 2
dic["Fran"] = 3
print(dic)

# Agora acessando o Dicionário pela chave e descobrindo o valor atribuído
dic["Fran"]

# Quando quero saber quais as chaves do dicionário
print(dic.keys())

# Quando quero ver os valores atribuídos as nossas chaves
print(dic.values())

# Descobrindo o tipo de retorno dá variável
type(dic.keys())

''' Comentário em bloco:
Quero saber qual a posição daquele valor atribuído, 
porém é algo que não consigo mexer, por isso precisamos 
encontrar um novo modo de fazer
'''
dic.keys()[0]

# Informo ao python que eu quero essa lista com a lista de chaves como resultado
list(dic.keys()) # Aparece todas as chaves
list(dic.keys())[0] # Aparece a chave na posição 0=1

# Quando eu quero junto os dois, tanto chave como valor de cada chave
list(dic.items()) # ele me retorna uma lista de tuplas, posso modificar o que tem dentro da lista mas não o que tem dentro das tuplas.

# Iniciando a abertura de arquivos no modo leitura
arquivoTexto = open("/content/drive/MyDrive/AulaPythonPLN/escrita.txt", "r")
arquivoLido = arquivoTexto.read() # Ao invés de só mostrar, estou salvando de uma variável, 
print(len(arquivoLido.split())) # Mas pra quem já tem o hábito pode só chamar o método
arquivoTexto.close()# Contei quantas palavras possui o texto incluindo ifens e acentuações

# Iniciando a abertura de arquivos no modo escrever
arquivoEscrita = open("/content/drive/MyDrive/AulaPythonPLN/escrita.txt", "w")
arquivoEscrita.write("Curso de PLN Aula 3")
arquivoEscrita.close()

# Testanto a edição do arquivo
arquivoEdicao = open("/content/drive/MyDrive/AulaPythonPLN/escrita.txt", "r")
arquivoEdicao.read()

# Testando mais uma edição antes de fechar
arquivoAdicao = open("/content/drive/MyDrive/AulaPythonPLN/escrita.txt", "w")
arquivoAdicao.write("Curso de PLN Aula 3 - Final")
arquivoAdicao.close()

# Testanto se a edição do arquivo foi efetuada
arquivoAdicao = open("/content/drive/MyDrive/AulaPythonPLN/escrita.txt", "r")
arquivoAdicao.read()
arquivoAdicao.close()

# Testando o appendice antes de fechar
arquivoAdicao = open("/content/drive/MyDrive/AulaPythonPLN/escrita.txt", "a")
arquivoAdicao.write(" Fim da terceira aula!")
arquivoAdicao.close()

# Testanto se o appendice do arquivo foi efetuada
arquivoAdicao = open("/content/drive/MyDrive/AulaPythonPLN/escrita.txt", "r")
arquivoAdicao.read()
arquivoAdicao.close()

# Testando o appendice com numero, forçando a transformar 10 em uma string antes de fechar
arquivoAdicao = open("/content/drive/MyDrive/AulaPythonPLN/escrita.txt", "a")
numero = 10
arquivoAdicao.write(str(numero))
arquivoAdicao.close()

# Testanto se o appendice do arquivo foi efetuada
arquivoAdicao = open("/content/drive/MyDrive/AulaPythonPLN/escrita.txt", "r")
arquivoAdicao.read()

"""#Aula 4 - NLTK"""

'aspas simples'

"aspas duplas"

""" Esse comentário de 
aspas triplas ignora
a quebra de libra """

# Exemplo com números, o uso do mais gera a soma
variante = 1 + 1
variante

# Entre strings gera concatenação
r = "Fran"
#r = r + " Antunes"
l = " Antunes"
r + l

'''Não é possível usar uma string e somar a um outro 
tipo de elemento, mas é possível forçar o python a fazer o cast'''
pi = 3.14
info = "O valor de pi é = " + str(pi)
print(info)

# Fatiando as strings
string = "Instituto de ciências matemáticas e de computação"
string[1:10]
string[:6]

# Colocando a frase anterior toda em maiúsculo
string.upper()

# substituir uma string com outra string. 
#obs: tem diferença entre maiúscula/minúsculas e letras acentuadas
string.replace("de", "X")

# Contando quantas palavras possui na minha frase
len(string.split())

# Separando a minha string em uma lista, quebrando a string
lista_split = string.split()
lista_split.append("ICMC") #Adicionando ao final da lista
print(lista_split)# Fiz uma geração de texto, atualizei a partir do split e o join

# Fazendo o movimento inverso do anterior, juntando a lista para uma string
juntar = " ".join(lista_split)
juntar

"""#Ferramentas - NLTK
Biblioteca de ferramentas úteis para a utilizações dos princípios de PLN


Linguagem - Python
"""

!pip install nltk #instalanso o NLTK ! PRECISA DESSA INFORMAÇÃO

import nltk

# Download do nosso pacote
nltk.download()

# Acessando um corpus pelo nltk, chamando a função nltk.
# dir(nltk.corpus.mac_morpho) mostra todas as funções que ele tem
# nltk.corpus.mac_morpho.words() Função words mostrou todas as palavras que eu tenho no morpho
len(nltk.corpus.mac_morpho.words()) # Mostrou quantas palavras eu tenho no texto.

# Separamos todo o texto em uma lista, agora cada palavra é uma posição na lista, separou por sentenças
nltk.corpus.mac_morpho.sents()[1] # separando apenas a segunda sentença

# Acessando as palavras anotadas e suas classe gramaticais
nltk.corpus.mac_morpho.tagged_words() # Lista de tuplas com as palavras etiquetadas e classe gramatical

# Acessando por sentença, uma lista de lista de tuplas de palavras com sua classe gramatical
nltk.corpus.mac_morpho.tagged_sents()[1] # [1] escolhendo a segunda sentença

# Fazendo a tokenização de um texto, é mais avançado que o split pq separa a vírgulas e pontos
nltk.word_tokenize("Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII.")

# Importando um módulo NLTK com uma classe específica para tokenizar sem pontuações.
from nltk.tokenize import RegexpTokenizer
texto = "Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII."
tokenizador = RegexpTokenizer(r'\w+') # r = informa que virá uma expressão regular dentro de aspas
tokens = tokenizador.tokenize(texto)
# print(tokens) # \w = combinação de caracteres que for letras e numeros o + é sequência, ou seja
tokens #uma ou mais ocorrências de caracteres.

# Importando um módulo NLTK com uma classe específica para tokenizar sem pontuaçõese números
from nltk.tokenize import RegexpTokenizer
tokenizador = RegexpTokenizer(r'[A-z]\w*')#[] todos os tokens que tem aqueles caracteres
tokens = tokenizador.tokenize(texto)
tokens

# Ordenando os tokens e sabendo as ocorrências no texto
texto = "Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII."
tokens = nltk.word_tokenize(texto)
frequencia = nltk.FreqDist(tokens)
frequencia.most_common(10) # As 10 pavras que mais possuem ocorrências no texto se não colocar parâmetro usa todas

"""O exemplo a seguir difere do anterior pelo seguinte: No código anterior primeiro eu tinha um texto que já tem nesse código e usei diretamente o tokenize do nltk, depois que ele fez a tokenização eu passei a lista de tokens pro FreqDist() que fez o calculo dessa freequência e eu mostrei essa frequência com o frequencia.most_common(10), o que mudou para o outro exemplo? A forma de tokenização, ao invés de usar o nlkt.word.tokenize() eu usei o RegexpTokenizer, importei o tokenize e crirei o tokenize usando essa expressão regular eu vou pegar todas as palavras, números e anderscore e envio o texto para o tokenizador, retornar os tokens somente de palavra e números."""

# Importando um módulo NLTK com uma classe específica para tokenizar e calcular frequencia sem pontuações mantendo números
from nltk.tokenize import RegexpTokenizer
tokenizador = RegexpTokenizer(r'\w+')#[] todos os tokens que tem aqueles caracteres
tokens = tokenizador.tokenize(texto)
frequencia = nltk.FreqDist(tokens)
frequencia.most_common()

"""Como fazer esse passo, quando utilizarmos um texto grande?"""

# Utilizando um arquivo
corpus = open('/content/drive/MyDrive/AulaPythonPLN/corpus_teste.txt').read()
print(corpus)

from nltk.tokenize import RegexpTokenizer
tokenizador = RegexpTokenizer(r'\w+')
tokens = tokenizador.tokenize(corpus)
frequencia = nltk.FreqDist(tokens)
frequencia.most_common()

"""Podemos também fazer com que palavaras como (a, de, na, no, e, o, do) palavras que ocorrem com muita frequencia mas que não tem muito utilizade para a tarefa que estamos realizando, é  uma maneira de tirar fazer com que não apareçam, existe um termo para essas palavras, chamadas de stopwords.

Podemos ainda analisar que algumas palavras aparecem com a inicial maiúscula e em momentos toda minúscula, justamente pq é início de sentença, mas para o tokenizador mesmo sendo a mesma palavra, contam separadamente, para isso é necessário utilizar o metodo de string lower() que ignora o maiusculo tratando as como iguais e trazendo assim melhorando nosso sistema de frequencia
"""

from nltk.tokenize import RegexpTokenizer

tokenizador = RegexpTokenizer(r'[a-zA-Z\w*')
tokens = tokenizador.tokenize(corpus)

nova_lista = []
for token in tokens:
  nova_lista.append(token.lower())

#print(nova_lista)
# Transformando todos os tokens da lista e colocando todos em minúsculos e adicionando a essa nova lista
#Adicionando a frequencia dessa nova lista frequencia = nltk.FreqDist(nova_lista)
frequencia.most_common()

# Resolvendo os problemas de stopwords, consideradas irrelevantes para um certo resultado buscado
stopwords = nltk.corpus.stopwords.words('portuguese')#serve para identificar em português essas palavras, já está imbutido no nltk

"""#List comprehension
É uma técnica de *list comprehension* é uma forma direferente e avançada de criar iuma lista. Não é obrigatório saber usá-la, mas é muito interessante conhecer sua construção.

O Python entende que é uma *list comprehension* quando criamos uma laço de repetição entre colchetes: [i for i in range (10)]. Essa construção criará a seguinte lista: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. Veja que é possível fazer isso sem essa contrução.

Uma forma genérica de imaginar uma *list comprehension* é montar a seguinte estrutura:


**<lista_final = [elemento_da_lista for elemento_da_lista in lista_de_elementos]>** 

Lembrando que você poderá acrescentar alguma condição para o elemento ser acrescentado na lista:

**<lista_final = [elemento_da_lista for elemento_da_lista in lista_de_elementos: if condição]>**
"""

from nltk.tokenize import RegexpTokenizer

tokenizador = RegexpTokenizer(r'[a-zA-Z\w*')
tokens = tokenizador.tokenize(corpus)

nova_lista = []
'''for token in tokens:
  if token.lower() not in stopwords:#Se token.lower nao estiver na lista de stopwords, adicionar a nova lista
    nova_lista.append(token.lower())'''
#print(nova_lista)
nova_lista = [token.lower() for token in token if token.lower() not in stopwords] #o for em uma linha
frequencia = nltk.FreqDist(nova_lista)
frequencia.most_common() #As palavras que aparecem são as que dominam o texto e o assunto a que ele se refere

s = 'instituto de ciências matemáticas e de computação'
print(len(s.split()))

"""#Aula 5 - N-Gramas e spaCy"""

#Utilizando ngramas, ajuda a entidar as entidades nomeadas
corpus = open('/content/drive/MyDrive/AulaPythonPLN/corpus_teste.txt').read()
print(corpus)

#Importando todas as ngramas de uma só vez
from nltk import bigrams
from nltk import trigrams
from nltk import ngrams

# Tokenizando meu texto corpus
tokens = nltk.word_tokenize(corpus)

tokens_brigrams = list(bigrams(tokens))#list() retorna a lista com os tokens independente do tamanho, sem ela só retorna o gerador, por causa do tamanho
#Retorna um gerador
tokens_bigrams

tokens_trigrams = list(trigrams(tokens))

tokens_trigrams

tokens_ngrams = list(ngrams(tokens, 4))

tokens_ngrams

#Bigramas
texto = "Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII. O resultado, uma das maiores zebras da história do Super Bowl, acabou com a temporada perfeita de Tom Brady e companhia, que esperavam fazer história ao levantar o troféu da NFL sem sofrer uma derrota no ano."
from nltk import bigrams
list(bigrams(tokens))#Já com o texto tokenizado

#Trigramas
texto = "Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII. O resultado, uma das maiores zebras da história do Super Bowl, acabou com a temporada perfeita de Tom Brady e companhia, que esperavam fazer história ao levantar o troféu da NFL sem sofrer uma derrota no ano."
from nltk import trigrams
list(trigrams(tokens))#Já com o texto tokenizado

#N-gramas (4 ou mais)
texto = "Com um passe de Eli Manning para Plaxico Burress a 39 segundos do fim, o New York Giants anotou o touchdown decisivo e derrubou o favorito New England Patriots por 17 a 14 neste domingo, em Glendale, no Super Bowl XLII. O resultado, uma das maiores zebras da história do Super Bowl, acabou com a temporada perfeita de Tom Brady e companhia, que esperavam fazer história ao levantar o troféu da NFL sem sofrer uma derrota no ano."
from nltk import ngrams
list(ngrams(tokens, 4))#Já com o texto tokenizado, passar parâmetro 4, token com 4 palavras e assim seucessivamente

"""# Reconhecer entidades nomeadas"""

from nltk import bigrams

bigramas = list(bigrams(tokens)
#Lembrando que os elementos dessa lista são tuplas de brigramas, não podemos modificar mas acessar os elementos dela

#Observando a seguencia das palavras dos ngramas para reconhecer entidades nomeadas, como pedir para que o python faça isso?

for bigrama in brigramas: #Ja acessa o primeiro nivel do indice dos meus bigramas
  if bigrama[0][0].isupper() and bigrama[1][0].isupper():#se a 1ª palavra[0] tiver a 1ª letra[0] e a 2ª palavra[1] tiver a 1ª letra[0] maiúscula(.isupper())
    print(bigrama)#imprima o bigrama

from nltk import trigrams

trigramas = list(trigrams(tokens)

for trigrama in trigramas:
  if trigrama[0][0].isupper() and trigrama[1][0].isupper() and trigrama[2][0].isupper():
    print(trigrama)
    #Ou seja se 1ª, 2ª e 3ª palavra desse trigrama iniciarem (1ª) letra maiúscula, imprima o trigrama, do contrário não imprime

"""#Etiquetadores - NLTK

O NLTK possui **dois corpus** que servem como base para o etiquetador em português: o **Floresta** e o **Mac Morpho.**
* Para o inglês já existe um etiquetador padrão treinado: o **nltk.pos_tag().**

Os etiquetadores passam primeiramente por uma fase de treinamento com sentenças presentes.
* Floresta: 9.266 sentenças etiquetadas
* Mac Morpho: 51.397 sentenças etiquetadas

Como resultados=, os etiquetadores retornam uma tupla ('palavra', 'classe gramatical')
* Na qual a classe gramatical depende do treinamento que é realizado.
"""

from nltk.corpus import mac_morpho
from nltk.tag import UnigramTagger #UnigramTagger etiqueta e trás a classe gramatical mais provável utilizando um token, verificando o corpus do mac_morpho
#utilizando a frequência daquela palavra

tokens = nltk.word_tokenize(corpus)

sentencas_treino = mac_morpho.tagged_sents()
etiquetador = UnigramTagger(sentencas_treino)
etiquetado = etiquetador.tag(tokens)
print(etiquetado)

"""Por ter de passar por uma fase de treinamento, tinham palavras que o etiquetador não conseguiu identificar e fazer a classificação.

Uma solução é pré-classificar todas as palavras do texto como substantivos **(N)** e depois treinar o etiquetador normalmente
* Usa o pacote **DefaultTagger**
"""

from nltk.corpus import mac_morpho
from nltk.tag import UnigramTagger
form nltk.tag import DefaultTagger

tokens = nltk.word_tokenize(corpus)

etiq_padrao = DefaultTagger('N')
sentencas_treino = mac_morpho.tagged_sents()
etiquetador = UnigramTagger(sentencas_treino, backoff=etiq_padrao) #backoof um parâmetro, caso não encontre o UnigramTagger, coloca tal substantivo
etiquetado = etiquetador.tag(tokens)
print(etiquetado)

"""Com os etiquetadores: MAC MORPHO
Podemos fazer várias manipulações com a lista de tuplas resultante:
* Análise descritivas
* Análise sintática
* **Chunking**
  * Reconhecimento de entidades nomeadas utilizando a classe **RegexParser** passando um padrão de expressão regular, com o padrão que queremos encontrar no texto, assim podemos desenhar a nossa árvore.
"""

from nltk.chunk import RegexParser

pattern = 'NP: {<NPROP><NPROP> | <N><N>}' #NP= sintague nominal, ou seja, um par de nome proprio(NPROP) ou um par de substantivos(N)
analise_gramatical = RegexParser(pattern)

arvore = analise_gramatical.parse(etiquetado)#Método que utilziamos para fazer a nossa análise, com o texto já etiquetado

"""#spaCy

É uma biblioteca python, que torna tudo um pouco mais simples, para processamento de textos.
* Escala industrial

Feito para uso em produção
* Criação de grandes aplicações que conseguem processar um grande volume de dados

Versão 3.0!
* O parser sintático mais rápido do mundo (!!)
* Acurácia de 92,6%
  * 1% a mais que o melhor parser disponível

Suporte para mais de 61 linguagens

Dados adicionais para a lematizaçao (Lembrando que não foi possível fazer com o NLTK porque só faz em inglês e no português sai errado.)
  * **pip install -U spacy-lookups-data**

Para que o spaCy consiga realizar sua funções é necessário que um modelo de linguagem esteja presente.
Modelos pré-treinados
* Entidades nomeadas
* Classes grmaticais
* Dependências sintáticas

Parecido com o corpus que utilizamos como treinamento no NLTK

Modelos de linguagem para o português

**python -m spacy download pt_core_news_sm** (pequeno)

**python -m spacy download pt_core_news_md** (médio)

**python -m spacy download pt_core_news_lg** (grande)

Todos eles são baseados no corpus WikiNER
* Vetores dos tokens e classes gramaticais
* Análise de dependência
* Entidades nomeadas

"""

# Só o pacote base
!pip install -U spacy

# Instalando pacotes adicionais lematização
!pip install -U spacy-lookups-data

# isntalando os modelos de linguas o grande
!python -m spacy download pt_core_news_lg

"""#spaCy - Uso

Necessário entender a diferença dos objetos **Doc** e **Token**:

Doc é uma sequência de objetos **Token**, ou seja, um documento com vários tokens manipuláveis. Métodos da classe **Doc** levam em consideração a manipulação desses tokens. Ex: Quantidade de tokens no documento.

Um **Token** é o token que aprendemos no NLTK: pode ser uma palavra, uma pontuação, numeral, espaços...
"""

import spacy 
# Vamos mandar o texto, não precisamos tokenizaar para enviar pra trabalhar com etiquetadores e afins
texto = "O Python é uma linguagem de programação 10 de alto nível e muito versátil. Ela suporta tanto a programação orientada a objetos quanto a programação estruturada. Com Python, você pode acessar bibliotecas nativas que oferecem funcionalidades para desenvolvimento de projetos e implementação de aplicações complexas. A tecnologia está presente nos códigos do Instagram, Netflix, Spotify, Reddit, Facebook, Google e muitos outros. Desde 1991."
nlp = spacy.load("pt_core_news_lg")
doc = nlp(texto) #o trexto, não os tokens!

"""A partir de agora vamos trabalhar com esse Doc que geramos na aba de código anterior.

Por meio desse documento podemos usar as funções mais interessantes do spaCy:
* Tokenização
* Stemmming e Lematizador
* Etiquetador
* Entidades Nomeadas


"""

# Aproveitando o texto anterior
tokens = [token for token in doc]
type(tokens[0]) #não é um string específica, se quiser recuperar a string específica do texto precisa usar um método específico.
#o doc não é um texto em si mas uma representação interna do spaCy que tem diversos métodos específicos e atributos

tokens = [token for token in doc]
tokens

tokens = [token.orth_ for token in doc] #agora em vez de ser um objeto qualquer ele retorna os texto em si dos tokens
tokens #É possível agora manipular essas palavras

"""Também podemos retornar os tipos de tokens, se é palavra, pontuação, numeral e etc.

somente palavras: **is_alpha**

Somente os números: **is_digit**

Somente pontuações: **is_punct**
"""

alpha_tokens = [token.orth_ for token in doc if token.is_alpha]
print("Alpha tokens: %s " % (alpha_tokens))

digit_tokens = [token.orth_ for token in doc if token.is_digit]
print("Digit tokens: %s " % (digit_tokens))

punct_tokens = [token.orth_ for token in doc if token.is_punct]
print("Punct tokens: %s " % (punct_tokens))

corpus = open('/content/drive/MyDrive/AulaPythonPLN/corpus_teste.txt').read()
corpus

import spacy #carrega p texto para modelo, carrega o modelo e basta manipular

nlp = spacy.load("pt_core_news_lg")
doc = nlp(corpus)

tokens = [token.orth_ for token in doc]
tokens

alpha_tokens = [token.orth_ for token in doc if token.is_alpha] #somente as palavras
print("Alpha tokens: %s " % (alpha_tokens))

digit_tokens = [token.orth_ for token in doc if token.is_digit] #somente os numeros
print("Digit tokens: %s " % (digit_tokens))

punct_tokens = [token.orth_ for token in doc if token.is_punct] #somente a pontuação
print("Punct tokens: %s " % (punct_tokens))

"""O spaCy não possui um stemmer padrão, porém tem um lematizador (o inverso do NLTK, pelo menos para o português)

Para lematizar basta usar o atributo **lema_** ele é feito automaticamente
"""

lemmas = [token.lemma_ for token in doc if token.pos_ == 'VERB'] #se a classe gramatical for verbo, representado pela etiqueta do verbo que é VERB
lemmas

# lista de tuplas com suas classes gramaticais utilizando o spaCy
import spacy
nlp = spacy.load("pt_core_news_lg") #modelo de rede neural, para fazer aprendizado de etiquetagem, utilizando a ideia de redes neurais
texto = "O Python é uma linguagem de programação 10 de alto nível e muito versátil. Ela suporta tanto a programação orientada a objetos quanto a programação estruturada. Com Python, você pode acessar bibliotecas nativas que oferecem funcionalidades para desenvolvimento de projetos e implementação de aplicações complexas. A tecnologia está presente nos códigos do Instagram, Netflix, Spotify, Reddit, Facebook, Google e muitos outros. Desde 1991."
etiquetas = [(token.orth_, token.pos_) for token in doc]
etiquetas

# Análise morfológica de cada token nos textos, o MORPH
morfologicas = [(token.orth_, token.morph) for token in doc]
morfologicas

#Identificando entidades nomeadas com o spaCy, basta usar a propriedade 'ents' na variável doc, direto no documento e não pelo token
entidades_nomeadas = list(doc.ents)
print(entidades_nomeadas)

detalhes_entidades = [(entidade, entidade.label_) for entidade in doc.ents]#buscando o tipo que é cada entidade, criando uma tupla onde
#o primeiro é uma entidade nomeada e o segundo é o tipo dessa entidade nomeada, caminhando pelo tempo
detalhes_entidades

"""É possível visualizar essas entidades nomeadas de forma gráfica, por meio do **displaCy**, é possível destacas todas as entidades nomeadas"""

html = spacy.displacy.render(doc, style="ent")
output_path = open("entidades_nomeadas.html", "w", encoding="utf-8")
output_path.write(html)
output_path.close()

"""O spaCy também faz a análise sintática, representação sintáticxa do texto. Utilizando o atributo **dep_** que retorna a dependência sintática do token em questão. Ou seja, mostra a relação entre os tokens"""

sintaxe = [(token.orth_, token.dep_) for token in doc]
print(sintaxe)

# visualizando a sintaxe
visualizar_sintaxe = spacy.displacy.render(doc, style='dep')
output_path = open("analise_dependencias.svg", "w", encoding="utf-8")
output_path.write(visualizar_sintaxe)
output_path.close()